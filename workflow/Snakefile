# Static Snakefile translated from tasks.json
from pathlib import Path
projectdir = Path(config["run"]["projectdir"])
GENOME =      str(projectdir / config["run"]["genome"])
INPUT_DIR =   str(projectdir / config["run"]["input"])
WORK_DIR =    str(projectdir / config["run"]["workdir"] / config["run"]["name"])
LOGS_DIR =    str(projectdir / config["run"]["logs"]    / config["run"]["name"] / "logs")
RESULTS_DIR = str(projectdir / config["run"]["results"] / config["run"]["name"])

#print the config file to results directory
config_file_1  = str(projectdir / config["run"]["workdir"] / config["run"]["name"] / "config.json")
config_file_2  = str(projectdir / config["run"]["results"] / config["run"]["name"] / "config.json")
#ignore file already created using contextlib
import contextlib
with contextlib.suppress(FileExistsError):
    Path(config_file_1).parent.mkdir(parents=True, exist_ok=True)
    Path(config_file_2).parent.mkdir(parents=True, exist_ok=True)

with open(config_file_1, "w") as f1, open(config_file_2, "w") as f2:
    import json
    json.dump(config, f1, indent=4)
    json.dump(config, f2, indent=4)

import glob
import re
# Dynamically read all .fastq files from INPUT_DIR
fastq_files = glob.glob(INPUT_DIR + "/*.fastq")
samples = sorted(set(Path(f) for f in fastq_files))
# Create symbolic links for samples to .forward.fastq and .reverse.fastq
for sample in samples:
    if ".forward.fastq" in str(sample) or ".reverse.fastq" in str(sample):
        continue
    #symlink to forward and reverse reads for _1, .1 _R1, _R2, .R1, .R2
    new = re.sub(r'(_1|\.1|_R1|\.R1|\_forward).fastq', '.forward.fastq', str(sample))    
    new = re.sub(r'(_2|\.2|_R2|\.R1|\_reverse).fastq', '.reverse.fastq', new)    
    with contextlib.suppress(FileExistsError):
        Path(new).symlink_to(sample)

# Get sample names from the filenames
fastq_files = glob.glob(INPUT_DIR + "/*.forward.fastq")
samples = sorted(set(Path(f).with_suffix('').stem for f in fastq_files))

rule all:
    input:
        expand(WORK_DIR+"/AlignReads/{sample}.sam", sample=samples),
        RESULTS_DIR+"/variant_calls/detectedSVs/insertions.bed"

rule BuildIndex:
    input:
        GENOME
    output:
        WORK_DIR+"/index.1.bt2",
        WORK_DIR+"/index.2.bt2",
        WORK_DIR+"/index.3.bt2",
        WORK_DIR+"/index.4.bt2",
        WORK_DIR+"/index.rev.1.bt2",
        WORK_DIR+"/index.rev.2.bt2",
    log:
        LOGS_DIR+"/build_index.log",
        time=LOGS_DIR+"/build_index.time",
        slurm=LOGS_DIR+"/build_index"
    container:
        "envs/bowtie2/bowtie2.sif"
    shell:
        "/usr/bin/time -o {log.time} -v " + \
        "bowtie2-build {input} "+WORK_DIR+"/index"

rule AlignReads:
    input:
        index=WORK_DIR+"/index.1.bt2",
        forward_read= INPUT_DIR + "/{sample}.forward.fastq",
        reverse_read= INPUT_DIR + "/{sample}.reverse.fastq"
    output:
        WORK_DIR+"/AlignReads/{sample}.sam"
    log:
        out=LOGS_DIR+"/AlignReads/align_reads.{sample}.log",
        err=LOGS_DIR+"/AlignReads/align_reads.{sample}.err",
        slurm=LOGS_DIR+"/AlignReads/align_reads.{sample}.slurm",
        time=LOGS_DIR+"/AlignReads/align_reads.{sample}.time"
    container:
        "envs/bowtie2/bowtie2.sif"
    shell:
        "/usr/bin/time -o {log.time} -v " + \
        "bowtie2 -x data/index -1 {input.forward_read} -2 {input.reverse_read} -S {output} 1> {log.out} 2> {log.err}"

rule SortAlignments:
    input:
        WORK_DIR+"/AlignReads/{sample}.sam"
    output:
        WORK_DIR+"/AlignReads/{sample}.sorted.sam"
    log:
        out=LOGS_DIR+"/AlignReads/sort_alignments.{sample}.log",
        err=LOGS_DIR+"/AlignReads/sort_alignments.{sample}.err",
        slurm=LOGS_DIR+"/AlignReads/sort_alignments.{sample}.slurm",
        time=LOGS_DIR+"/AlignReads/sort_alignments.{sample}.time"
    container:
        "envs/samtools/samtools.sif"
    shell:
        "/usr/bin/time -o {log.time} -v " + \
        "samtools sort {input} -O SAM -o {output} 1> {log.out} 2> {log.err}"

rule CalculateDepth:
    input:
        WORK_DIR+"/AlignReads/{sample}.sorted.sam"
    output:
        WORK_DIR+"/CalculateDepth/{sample}.depth"
    log:
        err=LOGS_DIR+"/CalculateDepth/{sample}.err",
        slurm=LOGS_DIR+"/CalculateDepth/{sample}.slurm",
        time=LOGS_DIR+"/CalculateDepth/{sample}.time"
    container:
        "envs/samtools/samtools.sif"
    shell:
        "/usr/bin/time -o {log.time} -v " + \
        "samtools depth -a {input} > {output} 2> {log.err}"

rule SumDepth:
    input:
        expand(WORK_DIR+"/CalculateDepth/{sample}.depth", sample=samples)
    output:
        WORK_DIR+"/CalculateDepth/total.depth"
    log:
        err=LOGS_DIR+"/CalculateDepth/sum_depth.err",
        slurm=LOGS_DIR+"/CalculateDepth/sum_depth.slurm",
        time=LOGS_DIR+"/CalculateDepth/sum_depth.time"
    shell:
        "/usr/bin/time -o {log.time} -v " + \
        "awk '{{sum=0; for(i=3; i<=NF; i+=3) sum+=$i; print $1, $2, sum}}' {input} > {output}"

rule FindRegionsOfInterest:
    input:
        depth=WORK_DIR+"/CalculateDepth/total.depth"
    output:
        WORK_DIR+"/FilterReads/regions_of_interest.txt"
    log:
        out=LOGS_DIR+"/FilterReads/find_regions_of_interest.out",
        err=LOGS_DIR+"/FilterReads/find_regions_of_interest.err",
        slurm=LOGS_DIR+"/FilterReads/find_regions_of_interest.slurm",
        time=LOGS_DIR+"/FilterReads/find_regions_of_interest.time"
    shell:
        "/usr/bin/time -o {log.time} -v " + \
        "GrassSV.py find_roi {input.depth} {output} 10 -m 150 1> {log.out} 2> {log.err}"

rule FilterRegions:
    input:
        forward_read= INPUT_DIR + "/{sample}.forward.fastq",
        reverse_read= INPUT_DIR + "/{sample}.reverse.fastq",
        alignments=WORK_DIR+"/AlignReads/{sample}.sam",
        alignments_sorted=WORK_DIR+"/AlignReads/{sample}.sorted.sam",
        regions_of_interest=WORK_DIR+"/FilterReads/regions_of_interest.txt"
    output:
        forward_reads=WORK_DIR+"/FilterReads/{sample}.filtered.forward.fastq",
        reverse_reads=WORK_DIR+"/FilterReads/{sample}.filtered.reverse.fastq"
    log:
        out=LOGS_DIR+"/FilterReads/filter_regions.{sample}.out",
        err=LOGS_DIR+"/FilterReads/filter_regions.{sample}.err",
        slurm=LOGS_DIR+"/FilterReads/filter_regions.{sample}.slurm",
        time=LOGS_DIR+"/FilterReads/filter_regions.{sample}.time"
    shell:
        "/usr/bin/time -o {log.time} -v " + \
        "GrassSV.py filter_reads -f1 {output.forward_reads} -f2 {output.reverse_reads} -s {input.alignments} -ss {input.alignments_sorted} -roi {input.regions_of_interest} 1> {log.out} 2> {log.err}"

rule AggregateFilteredReads:
    input:
        forward_reads=expand(WORK_DIR+"/FilterReads/{sample}.filtered.forward.fastq", sample=samples),
        reverse_reads=expand(WORK_DIR+"/FilterReads/{sample}.filtered.reverse.fastq", sample=samples)
    output:
        forward_reads=WORK_DIR+"/FilterReads/filtered.forward.fastq",
        reverse_reads=WORK_DIR+"/FilterReads/filtered.reverse.fastq"
    log:
        err=LOGS_DIR+"/aggregate_filtered_reads.err",
        slurm=LOGS_DIR+"/aggregate_filtered_reads.slurm"
    shell:
        "cat {input.forward_reads} > {output.forward_reads} 2> {log.err};"
        "cat {input.reverse_reads} > {output.reverse_reads} 2> {log.err};"

rule AssembleContigs:
    input:
        forward_reads=WORK_DIR+"/FilterReads/filtered.forward.fastq",
        reverse_reads=WORK_DIR+"/FilterReads/filtered.reverse.fastq"
    output:
        WORK_DIR+"/AssembleContigs/alga_contigs.fasta"
    log:
        out=LOGS_DIR+"/AssembleContigs/assemble_contigs.out",
        err=LOGS_DIR+"/AssembleContigs/assemble_contigs.err",
        slurm=LOGS_DIR+"/AssembleContigs/assemble_contigs.slurm",
        time=LOGS_DIR+"/AssembleContigs/assemble_contigs.time"
    container:
        "envs/ALGA/alga.sif"
    shell:
        "/usr/bin/time -o {log.time} -v " + \
        "ALGA --threads=8 --file1 {input.forward_reads} --file2 {input.reverse_reads} --output "+WORK_DIR+"/AssembleContigs/alga 1> {log.out} 2> {log.err}"

rule AlignContigs:
    input:
        contigs=WORK_DIR+"/AssembleContigs/alga_contigs.fasta",
        ref=GENOME
    output:
        WORK_DIR+"/AssembleContigs/quast/contigs_reports/all_alignments_alga_contigs.tsv",
        quast_dir=directory(WORK_DIR+"/AssembleContigs/quast")
    log:
        out=LOGS_DIR+"/AssembleContigs/align_contigs.out",
        err=LOGS_DIR+"/AssembleContigs/align_contigs.err",
        slurm=LOGS_DIR+"/AssembleContigs/align_contigs.slurm",
        time=LOGS_DIR+"/AssembleContigs/align_contigs.time"
    container:
        "envs/quast/quast.sif"
    shell:
        "/usr/bin/time -o {log.time} -v " + \
        "quast.py {input.contigs} -r {input.ref} -m 250 -o {output.quast_dir} 1> {log.out} 2> {log.err};"

rule CallVariants:
    input:
        quast_alignments=WORK_DIR+"/AssembleContigs/quast/contigs_reports/all_alignments_alga_contigs.tsv",
    output:
        variant_calls=directory(RESULTS_DIR+"/variant_calls"),
        insertions=RESULTS_DIR+"/variant_calls/detectedSVs/insertions.bed"
    log:
        out=LOGS_DIR+"/call_variants.out",
        err=LOGS_DIR+"/call_variants.err",
        slurm=LOGS_DIR+"/call_variants.slurm",
        time=LOGS_DIR+"/call_variants.time"
    shell:
        "/usr/bin/time -o {log.time} -v " + \
        "GrassSV.py find_sv {input.quast_alignments} -o {output.variant_calls} 1> {log.out} 2> {log.err};"